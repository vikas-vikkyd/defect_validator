{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "invalid_defect_reason_iedntifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAgpyKidk6YN"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vdu14vSm1-k"
      },
      "source": [
        "**Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37bJEKWOm4-N"
      },
      "source": [
        "max_len_desc = 100\n",
        "max_len_comm = 20\n",
        "num_words = 12000\n",
        "oov_token = '<oov>'\n",
        "padding = 'pre'\n",
        "truncating = 'pre'\n",
        "embedding_dim = 100"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5my1P6rlqy4"
      },
      "source": [
        "**Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUq90qYtliMt"
      },
      "source": [
        "def cleaning_coments(x):\n",
        "    x = re.sub(r'\\[.+\\]','', str(x))\n",
        "    x = re.sub(\"[?!@#$\\r\\n.:0123456789\\t-]\", '', x)\n",
        "    x = x.strip()\n",
        "    return x\n",
        "\n",
        "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
        "              \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\",\n",
        "              \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\",\n",
        "              \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\",\n",
        "              \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\",\n",
        "              \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\",\n",
        "              \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\",\n",
        "              \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\",\n",
        "              \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\",\n",
        "              \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\",\n",
        "              \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
        "def clean_description(x):\n",
        "    x = re.sub(\"[!@#$\\r\\n.:0123456789\\t-]\", '', x)\n",
        "    x = x.replace(\"*\", ' ').replace(\",\", ' ').replace(\"[\", '').replace(\"]\", '').replace(\"|\", '')\n",
        "    arr = x.split()\n",
        "    new_arr = [word for word in arr if not word in stopwords]\n",
        "    sentence = ' '.join(word for word in new_arr)\n",
        "    return sentence\n",
        "\n",
        "FEATURE_COLUMNS = ['Status', 'Description', 'Comments', 'Priority', 'RootCause', 'Reporter', \n",
        "           'IssueKey', 'Summary', 'Sprint', 'Component', 'Application', 'Assigne']\n",
        "def process_json_file(file_name, FEATURE_COLUMNS):\n",
        "    f = open(file_name)\n",
        "    Status = []\n",
        "    Description = []\n",
        "    Comments = []\n",
        "    Priority = []\n",
        "    RootCause = []\n",
        "    Reporter = []\n",
        "    IssueKey = []\n",
        "    Summary = []\n",
        "    Sprint = []\n",
        "    Component = []\n",
        "    Application = []\n",
        "    Assigne = []\n",
        "    data = json.load(f)\n",
        "    for i in data:\n",
        "        Status.append(i.get(FEATURE_COLUMNS[0], np.nan))\n",
        "        Description.append(i.get(FEATURE_COLUMNS[1], np.nan))\n",
        "        if len(i.get(FEATURE_COLUMNS[2], [])) !=0:\n",
        "            Comments.append(i.get(FEATURE_COLUMNS[2], [np.nan])[0]['body'])\n",
        "        else:\n",
        "            Comments.append(np.nan)\n",
        "        Priority.append(i.get(FEATURE_COLUMNS[3], np.nan))\n",
        "        RootCause.append(i.get(FEATURE_COLUMNS[4], np.nan))\n",
        "        Reporter.append(i.get(FEATURE_COLUMNS[5], np.nan))\n",
        "        IssueKey.append(i.get(FEATURE_COLUMNS[6], np.nan))\n",
        "        Summary.append(i.get(FEATURE_COLUMNS[7], np.nan))\n",
        "        Sprint.append(i.get(FEATURE_COLUMNS[8], np.nan))\n",
        "        Component.append(i.get(FEATURE_COLUMNS[9], np.nan))\n",
        "        Application.append(i.get(FEATURE_COLUMNS[10], np.nan))\n",
        "        Assigne.append(i.get(FEATURE_COLUMNS[11], np.nan))\n",
        "    f.close()\n",
        "    df = pd.DataFrame(data=Status, columns=['Status'])\n",
        "    df['Description'] = Description\n",
        "    df['Comments'] = Comments\n",
        "    df['Priority'] = Priority\n",
        "    df['RootCause'] = RootCause\n",
        "    df['Reporter'] = Reporter\n",
        "    df['IssueKey'] = IssueKey\n",
        "    df['Summary'] = Summary\n",
        "    df['Sprint'] = Sprint\n",
        "    df['Component'] = Component\n",
        "    df['Application'] = Application\n",
        "    df['Assigne'] = Assigne\n",
        "    return df\n",
        "\n",
        "def get_first_n_words_desc(x):\n",
        "    arr = x.split()\n",
        "    len_x = len(arr)\n",
        "    if len_x < max_len_desc:\n",
        "        return ' '.join(word for word in arr)\n",
        "    else:\n",
        "        new_arr = arr[:max_len_desc]\n",
        "        return ' '.join(word for word in new_arr)\n",
        "\n",
        "def get_first_n_words_comments(x):\n",
        "    arr = x.split()\n",
        "    len_x = len(arr)\n",
        "    if len_x < max_len_comm:\n",
        "        return ' '.join(word for word in arr)\n",
        "    else:\n",
        "        new_arr = arr[:max_len_comm]\n",
        "        return ' '.join(word for word in new_arr)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-7I9Mwdmfug"
      },
      "source": [
        "**Data Prep**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57fVVHSIml8x"
      },
      "source": [
        "df_cancelled = process_json_file('cancelled.json', FEATURE_COLUMNS)\n",
        "df_cancelled['Description'] = df_cancelled['Description'].apply(clean_description)\n",
        "df_cancelled = df_cancelled[['Description', 'Comments']]\n",
        "df_cancelled['Comments'] = df_cancelled['Comments'].apply(cleaning_coments)\n",
        "df_cancelled = df_cancelled[df_cancelled['Comments'] != 'nan']\n",
        "df_cancelled['Description'] = df_cancelled['Description'].apply(get_first_n_words_desc)\n",
        "df_cancelled['Comments'] = df_cancelled['Comments'].apply(get_first_n_words_comments)\n",
        "df_cancelled['all'] = df_cancelled['Description'] + ' ' +df_cancelled['Comments']\n",
        "\n",
        "#Create Tokenizer\n",
        "tokenizer_cancelled = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
        "tokenizer_cancelled.fit_on_texts(df_cancelled['all'].values)\n",
        "\n",
        "#Create Sequence\n",
        "sequences = df_cancelled['all'].values\n",
        "sequences = tokenizer_cancelled.texts_to_sequences(sequences)\n",
        "sequences = pad_sequences(sequences, maxlen=max_len_desc+max_len_comm, \n",
        "                          padding=padding, truncating=truncating)\n",
        "\n",
        "#Prepare Data for next word prediction\n",
        "train_data = []\n",
        "for seq in sequences:\n",
        "    start = 0\n",
        "    end = max_len_desc + 1\n",
        "    for i in range(max_len_comm):\n",
        "        dat = seq[start:end]\n",
        "        start = start + 1\n",
        "        end = end + 1\n",
        "        if len(set(dat)) != 1:\n",
        "            train_data.append(dat)\n",
        "            \n",
        "#Define sequence and labels\n",
        "sequences_x = []\n",
        "labels = []\n",
        "for seq in train_data:\n",
        "    sequences_x.append(seq[0:-1])\n",
        "    labels.append(seq[-1:])\n",
        "\n",
        "sequences_x = np.array(sequences_x)\n",
        "labels = np.array(labels)\n",
        "labels = labels.reshape(labels.shape[0])\n",
        "labels = tf.keras.utils.to_categorical(labels, num_words)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6MgJyA6oG7l"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIwhsyAo4veV",
        "outputId": "c51cc6bf-8bc1-4ef5-c502-7059f4513451"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=num_words+1, output_dim=embedding_dim, input_length=max_len_desc),\n",
        "    tf.keras.layers.SimpleRNN(64),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(num_words, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(sequences_x, labels, epochs=30, batch_size=64)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "683/683 [==============================] - 91s 132ms/step - loss: 7.1021 - accuracy: 0.0342\n",
            "Epoch 2/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 6.5190 - accuracy: 0.0604\n",
            "Epoch 3/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 6.0744 - accuracy: 0.1023\n",
            "Epoch 4/30\n",
            "683/683 [==============================] - 90s 132ms/step - loss: 5.6757 - accuracy: 0.1318\n",
            "Epoch 5/30\n",
            "683/683 [==============================] - 90s 132ms/step - loss: 5.3546 - accuracy: 0.1549\n",
            "Epoch 6/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 5.0704 - accuracy: 0.1753\n",
            "Epoch 7/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 4.8239 - accuracy: 0.1940\n",
            "Epoch 8/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 4.6002 - accuracy: 0.2100\n",
            "Epoch 9/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 4.3963 - accuracy: 0.2284\n",
            "Epoch 10/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 4.2010 - accuracy: 0.2455\n",
            "Epoch 11/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 4.0233 - accuracy: 0.2617\n",
            "Epoch 12/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 3.8536 - accuracy: 0.2778\n",
            "Epoch 13/30\n",
            "683/683 [==============================] - 90s 132ms/step - loss: 3.7003 - accuracy: 0.2948\n",
            "Epoch 14/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 3.5535 - accuracy: 0.3137\n",
            "Epoch 15/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 3.4287 - accuracy: 0.3262\n",
            "Epoch 16/30\n",
            "683/683 [==============================] - 90s 133ms/step - loss: 3.2968 - accuracy: 0.3420\n",
            "Epoch 17/30\n",
            "683/683 [==============================] - 90s 131ms/step - loss: 3.1751 - accuracy: 0.3590\n",
            "Epoch 18/30\n",
            "683/683 [==============================] - 90s 132ms/step - loss: 3.0565 - accuracy: 0.3721\n",
            "Epoch 19/30\n",
            "683/683 [==============================] - 90s 131ms/step - loss: 2.9587 - accuracy: 0.3854\n",
            "Epoch 20/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 2.8777 - accuracy: 0.3953\n",
            "Epoch 21/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 2.7725 - accuracy: 0.4112\n",
            "Epoch 22/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 2.6971 - accuracy: 0.4245\n",
            "Epoch 23/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 2.6256 - accuracy: 0.4346\n",
            "Epoch 24/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 2.5590 - accuracy: 0.4484\n",
            "Epoch 25/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 2.4935 - accuracy: 0.4537\n",
            "Epoch 26/30\n",
            "683/683 [==============================] - 90s 131ms/step - loss: 2.4232 - accuracy: 0.4649\n",
            "Epoch 27/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 2.3634 - accuracy: 0.4750\n",
            "Epoch 28/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 2.3207 - accuracy: 0.4804\n",
            "Epoch 29/30\n",
            "683/683 [==============================] - 89s 131ms/step - loss: 2.2686 - accuracy: 0.4913\n",
            "Epoch 30/30\n",
            "683/683 [==============================] - 89s 130ms/step - loss: 2.2233 - accuracy: 0.4992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgCbusBft2nc"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TLqJaZizNn2"
      },
      "source": [
        "model.save(\"reason_generator.h5\")\n",
        "with open('tokenizer_cancelled.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer_cancelled, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "G8pBB11iL5JS",
        "outputId": "3663116f-ceb6-417e-ce19-89782009f656"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('tokenizer_cancelled.pickle') "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b34c6560-6495-40dd-979b-0d992b00fe60\", \"tokenizer_cancelled.pickle\", 550293)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwNW8KfWzqyV"
      },
      "source": [
        "**Next Word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5qroo-NoK9A",
        "outputId": "55a690ab-e3fd-46ff-90e7-2d9d4f5288a9"
      },
      "source": [
        "#Get next words\n",
        "seed_text = \"Steps reproduce Launch url https//qavirginvoyagescom/booking User done voyage selection User \\\n",
        "clicked Choose Cabin User entered access key details Access Key First Name Last Name Email Address click \\\n",
        "continue buttonUser summary pageuser changed currency USD GBP url stringObserve currency Summary page navigate \\\n",
        "confirmation page Expected Result Currency Should not Display GBP currency user not able complete Booking Actual \\\n",
        "Result Currency Displaying GBP currency user able complete booking details reflected saleforce refer attached \\\n",
        "recording\"\n",
        "next_words = 20\n",
        "word_dict = dict((value, key) for (key, value) in tokenizer_cancelled.word_index.items())\n",
        "for i in range(next_words):\n",
        "    seed_sentence = [seed_text]\n",
        "    seed_sequence = tokenizer_cancelled.texts_to_sequences(seed_sentence)\n",
        "    if len(seed_sequence[0]) > max_len_desc:\n",
        "        seed_sequence = [list(seed_sequence[0][(len(seed_sequence[0])-max_len_desc):])]\n",
        "    padded_seed_sequence = pad_sequences(seed_sequence, truncating=truncating, padding=padding, maxlen=max_len_desc)\n",
        "    seed_text = seed_text + \" \" + word_dict[np.argmax(model.predict(padded_seed_sequence)[0])]\n",
        "print(' '.join(word for word in seed_text.split()[-20:]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we have a duplicate asap ticket please find the correct notes and blocked to be the issue of metas can\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbV4XyfdM1Uj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}